{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "cats = pickle.load(open('./all_cats.pickle', 'rb'))\n",
    "for cnt, cat in enumerate(cats):\n",
    "    ind = np.isfinite(cat['eps'])\n",
    "    lambda_e.extend( cat['lambda_r'][ind])\n",
    "    eps.extend(cat['eps'][ind])\n",
    "\n",
    "    lambda_c.extend( cat['lambda_r'][ind])\n",
    "    stellarmass.extend( cat['mstar'][ind])\n",
    "    reff.extend( cat['reff'][ind])\n",
    "    ids.extend( cat['id'][ind] + cnt*10000) # annotation!\n",
    "    d2t.extend( cat['d2t'][ind])\n",
    "\n",
    "    rank.extend( 100*(np.argsort(cat['mstar'][ind])/sum(ind) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import utils.sampling as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import tree\n",
    "import pickle\n",
    "import tree.halomodule as hmo\n",
    "import numpy as np\n",
    "from analysis.misc import load_cat\n",
    "import scipy.stats\n",
    "import tree.ctutils as ctu\n",
    "#from analysis.evol_lambda_HM import MainPrg\n",
    "import draw\n",
    "import load\n",
    "from utils import match\n",
    "from analysis.all_plot_modules import *\n",
    "\n",
    "\n",
    "def extract_main_tree(t, idx, fatherID, fatherMass):\n",
    "    t_now = t[idx]\n",
    "    nstep = t_now[\"nstep\"]\n",
    "    nouts = [nstep]\n",
    "    atree = np.zeros(nstep + 1, dtype=t.dtype)\n",
    "    atree[0]=t_now\n",
    "    \n",
    "    for i in range(1, nstep + 1):\n",
    "        try:\n",
    "            #print(t[\"flist_index\"][idx])\n",
    "            #id_father = fatherID[t[\"flist_index\"][idx]]\n",
    "            id_father = fatherID[idx]\n",
    "            #print(id_father)\n",
    "            #print(len(id_father))\n",
    "            if len(id_father) > 1:\n",
    "                #mass_father = fatherMass[t[\"flist_index\"][idx]]\n",
    "                mass_father = fatherMass[idx]\n",
    "                #print(mass_father)\n",
    "                id_father = id_father[np.argmax(mass_father)]\n",
    "                ind_father = id_father[id_father > 0] -1\n",
    "                \n",
    "                nstep -= 1\n",
    "                t_father = t[np.where(t[\"nstep\"] == nstep)[0]][ind_father]\n",
    "                idx = t_father[\"idx\"]\n",
    "                #print(idx)\n",
    "                atree[i]=t_father\n",
    "                nouts.append(nstep)\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "    return atree\n",
    "\n",
    "\n",
    "def find_closest(A, target):\n",
    "    #A must be sorted\n",
    "    idx = A.searchsorted(target)\n",
    "    idx = np.clip(idx, 1, len(A)-1)\n",
    "    left = A[idx-1]\n",
    "    right = A[idx]\n",
    "    idx -= target - left < right - target\n",
    "    return idx\n",
    "\n",
    "\n",
    "def nout2nstep(data, nout):\n",
    "    return data[\"nstep\"][np.where(data[\"nout\"] == nout)]\n",
    "\n",
    "def nstep2nout(data, nstep):\n",
    "    try:\n",
    "        len(nstep)\n",
    "        import utils.match as mtc\n",
    "        ind = mtc.match_list_ind(data[\"nstep\"], nstep)\n",
    "    except:\n",
    "        ind = np.where(data[\"nstep\"] == nstep)[0]\n",
    "    return data[\"nout\"][ind]\n",
    "\n",
    "def zred2nout(data, nout):\n",
    "    return data[\"nstep\"][np.where(data[\"nout\"] == nout)]\n",
    "\n",
    "\n",
    "\n",
    "class MainPrg_old():\n",
    "    def __init__(self, atree, is_root=False):\n",
    "        \"\"\"\n",
    "            Separate tree data and lambda catalog data. \n",
    "            \n",
    "            early snapshot first.\n",
    "        \"\"\"\n",
    "        self.nsteps = atree[\"nstep\"]\n",
    "        self.idxs = atree[\"idx\"]\n",
    "        self.ids = atree[\"id\"]\n",
    "        self.nouts = nnza[\"nout\"]#nstep2nout(nnza, self.nsteps)\n",
    "        self.zreds = atree[\"zred\"]\n",
    "        self.aexps = 1/(1+self.zreds)\n",
    "        self.root_idx = atree[\"idx\"][0]\n",
    "        \n",
    "    def initialize_data(self, cat, force=False):\n",
    "        if hasattr(self, \"data\"):\n",
    "            if not force:\n",
    "                print(\"self.data already exists. use force=True to re-initialize it.\")\n",
    "                pass\n",
    "        else:\n",
    "            self.data=np.zeros(len(self.nsteps),\n",
    "                                  dtype=cat.dtype)\n",
    "\n",
    "class MainPrg():\n",
    "    def __init__(self, idx, nnza):\n",
    "        \"\"\"\n",
    "            Separate tree data and lambda catalog data. \n",
    "            \n",
    "            early snapshot first.\n",
    "        \"\"\"\n",
    "        self.root_idx = idx\n",
    "        self.nsteps = nnza[\"nstep\"] # truncate later.\n",
    "        #self.idxs = np.zeros(len(nnza), dtype=int)\n",
    "        #self.ids = np.zeros(len(nnza), dtype=int)\n",
    "        self.nouts = nnza[\"nout\"]\n",
    "        self.zreds = nnza[\"zred\"]\n",
    "        self.aexps = 1/(1+self.zreds)       \n",
    "        \n",
    "    def initialize_data(self, cat, force=False):\n",
    "        if hasattr(self, \"data\"):\n",
    "            if not force:\n",
    "                print(\"self.data already exists. use force=True to re-initialize it.\")\n",
    "                pass\n",
    "        else:\n",
    "            self.data=np.zeros(len(self.nsteps),\n",
    "                                  dtype=cat.dtype)\n",
    "\n",
    "    def set_data(self, cat, nout):\n",
    "        ind = np.where(cat[\"tree_root_id\"] == self.root_idx)[0]\n",
    "        if len(ind) == 1:\n",
    "            #self.data\n",
    "            inout = np.where(self.nouts == nout)[0]\n",
    "            if len(inout) == 1:\n",
    "                self.data[inout] = cat[ind]\n",
    "                #self.id[inout] = cat[ind][\"id\"]\n",
    "    \n",
    "    def fill_missing_data(self):\n",
    "        assert (self.ids[0] != 0)\n",
    "        # position angles cannot be linearly interpolated.\n",
    "        # skip.\n",
    "        #\n",
    "        # position and velocity are also not that linear..\n",
    "        # but let me just interpolate them.\n",
    "        # \n",
    "        # excluded=[\"lambda_arr2\"]\n",
    "        filled_fields = [\"eps\", \"epsh\", \"epsq\", \"lambda_12kpc\",\n",
    "                         \"lambda_arr\", \"lambda_arrh\",\n",
    "                         \"lambda_r\",\"lambda_r12kpc\",\n",
    "                         \"lambda_r2\",\"lambda_rh\",\"mgas\",\"mrj\",\"mstar\",\n",
    "                         \"reff\",\"reff2\",\"rgal\",\"rgal2\",\"rscale_lambda\",\n",
    "                         \"sfr\",\"sma\",\"smah\",\"smaq\",\"smi\",\"smih\",\"smiq\",\"ssfr\",\n",
    "                         \"vxc\", \"vyc\", \"vzc\", \"xc\", \"yc\", \"zc\"]\n",
    "\n",
    "        i_good_max = max(np.where(self.data[\"reff\"] > 0)[0])\n",
    "        i_bad = np.where(self.data['idx'] == 0)[0]\n",
    "        i_bad = i_bad[i_bad < i_good_max]\n",
    "        if len(i_bad) > 0:\n",
    "            for field in filled_fields:\n",
    "                # do not modify index and id fields.\n",
    "                arr = self.data[field] # it's a view.\n",
    "\n",
    "                for i_b in i_bad:\n",
    "                    # neighbouring array might also be empty. Search for closest valid element.\n",
    "                    # left point\n",
    "                    i_l = i_b - 1 \n",
    "                    while(i_l in i_bad):\n",
    "                        i_l = i_l - 1 \n",
    "                    # right point\n",
    "                    i_r = i_b + 1 \n",
    "                    while(i_r in i_bad):\n",
    "                        i_r = i_r + 1 \n",
    "                    arr[i_b] = (arr[i_b -1] + arr[i_b +1])/2.\n",
    "    \n",
    "    def truncate(self):\n",
    "        imax = np.where(self.data[\"lambda_r\"] > 0)[0] + 1\n",
    "        self.nsteps = self.nsteps[:imax]\n",
    "        self.nouts = self.nouts[:imax]\n",
    "        self.zreds = self.zreds[:imax]\n",
    "        self.aexps = self.aexps[:imax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "do_lambda_vs_e = True\n",
    "do_scatter = True\n",
    "sami_scatter = False\n",
    "load_data = False\n",
    "\n",
    "eps = []\n",
    "lambda_c = []\n",
    "lambda_e = []\n",
    "stellarmass =[]\n",
    "reff = []\n",
    "ids = []\n",
    "d2t = []\n",
    "\n",
    "rank = []\n",
    "\n",
    "annotate=False # output id of flat slow rotators\n",
    "color = False # BCGs in different colors.\n",
    "nout_fi = 37\n",
    "\n",
    "if load_data:\n",
    "    mpgs = pickle.load(open(\"main_prgs_final_augmented_5_10_0.5_0.5_0.5_37_0.01_filtered_.pickle\", \"rb\"))\n",
    "    eps = []\n",
    "    lambda_e = []\n",
    "\n",
    "    inout = 0 # nout=187\n",
    "    mstar_cut_hard = 5e9\n",
    "\n",
    "    for gal in mpgs:\n",
    "        mgal = gal.data[\"mstar\"][0]\n",
    "        if mgal > mstar_cut_hard:\n",
    "            eps.append(gal.data['eps'][inout])\n",
    "            lambda_e.append(gal.data['lambda_r'][inout])    \n",
    "else:\n",
    "    fname_vs_e = \"./figs/lambda_vs_e_HM_z1\"\n",
    "    nout_fi = [782, 358, 125][0]\n",
    "    cat_dir = [\"easy_final/\"]              \n",
    "    clusters = [['07206', '04466', '01605', \\\n",
    "                '35663', '24954', '49096', \\\n",
    "                '05427', '05420', '29172', \\\n",
    "                '29176', '10002', '36415', \\\n",
    "                '06098', '39990', '36413', \\\n",
    "                '17891'],[0,1,2,3,4,6,7,8,9,10,11,15,16]][1]\n",
    "                #[0,1,2,3,4,6,7,8,9,10,11,12,13,15,16,17,18,19,20]][1]\n",
    "    clusters = [str(i) for i in clusters]\n",
    "    #clusters = [\"05420\"]\n",
    "                # 35663, 49096, 249554, and 14172 to be added.\n",
    "\n",
    "    from analysis import misc\n",
    "    for cnt, cluster in enumerate(clusters):\n",
    "        #print(cluster)\n",
    "        #fname = cluster + \"/\" + cat_dir + \"catalog\" + str(nout_fi) + \".pickle\"\n",
    "        fname = \"Horizon-AGN/lambda_results/\" + cluster + \"/\" + \"catalog\" + str(nout_fi) + cluster + \".pickle\"\n",
    "        cat = misc.load_cat(fname)\n",
    "        cat = cat[~np.isnan(cat['eps'])]\n",
    "\n",
    "        ind = np.isfinite(cat['eps'])\n",
    "        lambda_e.extend(cat['lambda_r'][ind])\n",
    "        eps.extend(cat['eps'][ind])\n",
    "\n",
    "        lambda_c.extend(cat['lambda_r'][ind])\n",
    "        stellarmass.extend(cat['mstar'][ind])\n",
    "        reff.extend(cat['reff'][ind])\n",
    "        ids.extend(cat['id'][ind] + cnt*10000) # annotation!\n",
    "        d2t.extend(cat['d2t'][ind])\n",
    "\n",
    "        rank.extend(100*(np.argsort(cat['mstar'][ind])/sum(ind)))\n",
    "        #print(cat['eps'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6852"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(eps))\n",
    "i = np.isfinite(lambda_e)\n",
    "print(sum(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of galaxies in total 7301\n",
      "Number of galaxies below the demarkation line: 506\n"
     ]
    }
   ],
   "source": [
    "# load SAMI data\n",
    "sd = np.genfromtxt('./samidata/jvds_sami_lambdar_220216_0arc_fwhm.txt',\n",
    "                    dtype=(int, float, float, float, float, float, float),\n",
    "                    usecols=(0,2,3,6,7,10,11))\n",
    "sd.dtype.names = ('id', 'r05', 'e05', 'r1', 'e1', 'r2', 'e2') # e = errors, ellip = ellipticity\n",
    "\n",
    "dd = np.genfromtxt('./samidata/input_catalog_lambdar.txt',\n",
    "                          dtype=(int, float, float, float),\n",
    "                          usecols=(0,3,4,6))\n",
    "dd.dtype.names = ('id', 're', 'ellp', 'mstar')\n",
    "\n",
    "common_ids = np.intersect1d(sd['id'], dd['id'])\n",
    "\n",
    "ind = match.match_list_ind(common_ids, sd['id'])\n",
    "sd = sd[ind]\n",
    "ind = match.match_list_ind(common_ids, dd['id'])\n",
    "dd = dd[ind]\n",
    "\n",
    "from numpy.lib.recfunctions import append_fields\n",
    "\n",
    "if all(sd['id'] == dd['id']):\n",
    "    tt = dd[['re', 'ellp', 'mstar']]\n",
    "    sami_data = append_fields(sd, tt.dtype.names, [tt[n] for n in tt.dtype.names])\n",
    "    # only valid values\n",
    "    all_finite = np.all([np.isfinite(sami_data[field]) for field in ['id', 'r05', 'r1', 're', 'ellp', 'mstar']], axis=0)\n",
    "    sami_data = sami_data[all_finite]\n",
    "    \n",
    "# exclude ellp == 0\n",
    "\n",
    "sami_data = sami_data[sami_data['ellp'] > 0].data\n",
    "\n",
    "# Mass cut\n",
    "sami_data = sami_data[sami_data['mstar'] > 9.7]\n",
    "\n",
    "\n",
    "atlas = np.genfromtxt('./ATLAS3D/Emsellem2011_Atlas3D_Paper3_TableB1.txt',\n",
    "                      skip_header=12,\n",
    "                      usecols=(2,7))\n",
    "\n",
    "x = np.array(eps)[i] # isfinit\n",
    "y = np.array(lambda_e)[i]\n",
    "print(\"number of galaxies in total\", len(eps))\n",
    "print(\"Number of galaxies below the demarkation line:\", sum(y < 0.31 * np.sqrt(x)))\n",
    "\n",
    "#new_map = truncate_colormap(\"winter_r\", minval=0.3)\n",
    "\n",
    "# Color\n",
    "#hex_colors = ['#4c72b0', '#55a868', '#c44e52', '#8172b2', '#ccb974', '#64b5cd']\n",
    "twocolors=['#4c72b0', '#c44e52']\n",
    "#twocolors=[\"blue\", \"red\"]\n",
    "den_cmap=[\"cool\", \"YlGnBu\", \"PuBu\"][1]\n",
    "#blue = '#4c72b0'\n",
    "#red = '#c44e52'\n",
    "do_plot(x,y, atlas,\n",
    "        do_scatter=False,\n",
    "        contour_label=False,\n",
    "        surf = False,\n",
    "        img_scale = 2.0,\n",
    "        twocolors=twocolors,\n",
    "        den_cmap = \"YlGnBu_r\",\n",
    "        d_alpha=1.0,\n",
    "        levels=None,#np.linspace(0.02, 1.0, 19),\n",
    "        fname_vs_e = \"./figs/lambda_vs_e_HM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Halo.set_info] Couldn't load info file.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Horizon-AGN/Horizon-noAGN/snapshots/output_00782/info_00782.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-91833b71faac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhalomodule\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhmo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[0mallgal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhmo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHalo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m782\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_gal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoseung/Work/pyclusterevol/tree/halomodule.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, nout, base, info, halofinder, load, is_gal, return_id, outdir, verbose)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"none\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhalofinder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoseung/Work/pyclusterevol/tree/halomodule.py\u001b[0m in \u001b[0;36mset_info\u001b[1;34m(self, info)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[Halo.set_info] Couldn't load info file.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoseung/Work/pyclusterevol/tree/halomodule.py\u001b[0m in \u001b[0;36m_load_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[Halo.load_info] loading info\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[Halo.load_info] nout = {}, base ={}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[Halo.load_info] info is loaded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoseung/Work/pyclusterevol/load/info.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, nout, base, fn, load, data_dir, cosmo)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoseung/Work/pyclusterevol/load/info.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(self, nout, base, fn)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# sets ncpu_tot,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoseung/Work/pyclusterevol/load/info.py\u001b[0m in \u001b[0;36mread_info\u001b[1;34m(self, base, nout, verbose)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[1;34m\"\"\" backward compatibility. but use .load instead\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoseung/Work/pyclusterevol/load/info.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, base, nout, verbose)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m             \u001b[0marr1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0marr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Horizon-AGN/Horizon-noAGN/snapshots/output_00782/info_00782.txt'"
     ]
    }
   ],
   "source": [
    "from analysis.misc import load_cat\n",
    "from analysis.all_plot_modules import *\n",
    "from MajorMinorAccretion_module import *\n",
    "\n",
    "wdir = './Horizon-AGN/Horizon-noAGN/'\n",
    "#nout_fi = 782\n",
    "nout_fi=323\n",
    "\n",
    "nnza = np.genfromtxt(wdir + \"nout_nstep_zred_aexp.txt\",\n",
    "                     dtype=[(\"nout\", int),\n",
    "                            (\"nstep\", int),\n",
    "                            (\"zred\", float),\n",
    "                            (\"aexp\", float)])\n",
    "\n",
    "load_init = False\n",
    "if load_init:\n",
    "    mpgs = pickle.load(open(wdir + \"MPGS_init.pickle\", \"rb\"))\n",
    "else:\n",
    "    mpgs=[]\n",
    "    #samples = [0,1,2,3,4,6,7,8,9,10,11,12,13,15,16,17,18,19,20]\n",
    "    samples = [0,1,2,3,4,6,10,11,12,13,15]\n",
    "    for sample in samples:\n",
    "        # initialize mpgs\n",
    "        ss = str(sample)\n",
    "        f_cat = load_cat(wdir + 'lambda_results/' + ss + '/catalog' + str(nout_fi) + ss +'.pickle')\n",
    "        root_idx_all = f_cat['idx'][f_cat[\"idx\"] > 0].astype(int) # why idx are float???\n",
    "        for i, idx in enumerate(root_idx_all):\n",
    "            #atree = extract_main_tree(tt, idx, fatherID, fatherMass)\n",
    "            mpgs.append(MainPrg(idx, nnza))\n",
    "\n",
    "        # assign lambda measurement data\n",
    "        for nout in nnza[\"nout\"][:1]:\n",
    "            cat = load_cat(wdir + 'lambda_results/' + ss + '/catalog' + str(nout) + ss +'.pickle')\n",
    "            #print(nout)\n",
    "            for gal in mpgs:\n",
    "                if nout == nout_fi:\n",
    "                    gal.initialize_data(cat, force=True)\n",
    "                gal.set_data(cat, nout)\n",
    "        \n",
    "    for gal in mpgs:\n",
    "        gal.ids = gal.data[\"id\"]\n",
    "        gal.idxs = gal.data[\"idx\"]\n",
    "        #self.ids = np.zeros(len(nnza), dtype=int)\n",
    "        \n",
    "        \n",
    "from tree import halomodule as hmo\n",
    "allgal = hmo.Halo(nout=782, is_gal=True, base=wdir)\n",
    "\n",
    "import load\n",
    "info = load.info.Info(nout=782, base=wdir)\n",
    "\n",
    "galidlist = []\n",
    "for gal in mpgs:\n",
    "    #print(gal.ids)\n",
    "    #plt.plot(np.log10(gal.data[\"mstar\"]))\n",
    "    galidlist.append(gal.ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xall = allgal.data[\"x\"]\n",
    "yall = allgal.data[\"y\"]\n",
    "zall = allgal.data[\"z\"]\n",
    "\n",
    "bins = np.linspace(0, 1, 20)\n",
    "\n",
    "xbin = np.digitize(xall, bins)\n",
    "ybin = np.digitize(yall, bins)\n",
    "zbin = np.digitize(zall, bins)\n",
    "\n",
    "dist_cut = 5/info.pboxsize\n",
    "\n",
    "d5 = []\n",
    "N5 = []\n",
    "for idgal in galidlist:\n",
    "#for igal in range(len(allgal.data)):\n",
    "    igal = np.where(allgal.data[\"id\"] == idgal)[0]\n",
    "    x_this = allgal.data[\"x\"][igal]\n",
    "    y_this = allgal.data[\"y\"][igal]\n",
    "    z_this = allgal.data[\"z\"][igal]\n",
    "    \n",
    "    # get subsample to speed up the code\n",
    "    xb_this = xbin[igal]\n",
    "    yb_this = ybin[igal]\n",
    "    zb_this = zbin[igal]\n",
    "    \n",
    "    first_candidates = allgal.data[(np.abs(xbin - xb_this) < 2) \\\n",
    "                                  * (np.abs(ybin - yb_this) < 2)\\\n",
    "                                  * (np.abs(zbin - zb_this) < 2)]\n",
    "    \n",
    "    dist = np.sqrt(np.square(first_candidates[\"x\"] - x_this) + \n",
    "                   np.square(first_candidates[\"y\"] - y_this) + \n",
    "                   np.square(first_candidates[\"z\"] - z_this))\n",
    "    #print(len(dist))\n",
    "    N5.append(sum(dist < dist_cut))\n",
    "    d5.append(np.sort(dist)[4] * info.pboxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d5_80 = d5[np.argsort(d5)[np.ceil(0.8 * len(d5)).astype(int)]]\n",
    "i_isol = np.where(d5 >= d5_80)[0]\n",
    "isolated = [mpgs[i] for i in i_isol]\n",
    "\n",
    "d5_20 = d5[np.argsort(d5)[np.ceil(0.2 * len(d5)).astype(int)]]\n",
    "i_dense = np.where(d5 <= d5_20)[0]\n",
    "dense = [mpgs[i] for i in i_dense]\n",
    "\n",
    "eps_a = np.array([gal.data[\"eps\"][0] for gal in mpgs])\n",
    "lambda_a = np.array([gal.data[\"lambda_r\"][0] for gal in mpgs])\n",
    "\n",
    "eps_i = np.array([gal.data[\"eps\"][0] for gal in isolated])\n",
    "lambda_i = np.array([gal.data[\"lambda_r\"][0] for gal in isolated])\n",
    "\n",
    "eps_d = np.array([gal.data[\"eps\"][0] for gal in dense])\n",
    "lambda_d = np.array([gal.data[\"lambda_r\"][0] for gal in dense])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10650 2131 2130\n"
     ]
    }
   ],
   "source": [
    "print(len(eps_a), len(eps_d), len(eps_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_plot(eps_a, lambda_a, atlas,\n",
    "        do_scatter=False,\n",
    "        contour_label=False,\n",
    "        surf = False,\n",
    "        img_scale = 2.0,\n",
    "        twocolors=twocolors,\n",
    "        den_cmap = \"YlGnBu_r\",\n",
    "        d_alpha=1.0,\n",
    "        levels=None,#np.linspace(0.02, 1.0, 19),\n",
    "        fname_vs_e = \"./figs/lambda_vs_e_allEnv\")\n",
    "\n",
    "do_plot(eps_i, lambda_i, atlas,\n",
    "        do_scatter=False,\n",
    "        contour_label=False,\n",
    "        surf = False,\n",
    "        img_scale = 2.0,\n",
    "        twocolors=twocolors,\n",
    "        den_cmap = \"YlGnBu_r\",\n",
    "        d_alpha=1.0,\n",
    "        levels=None,#np.linspace(0.02, 1.0, 19),\n",
    "        fname_vs_e = \"./figs/lambda_vs_e_isol\")\n",
    "\n",
    "do_plot(eps_d, lambda_d, atlas,\n",
    "        do_scatter=False,\n",
    "        contour_label=False,\n",
    "        surf = False,\n",
    "        img_scale = 2.0,\n",
    "        twocolors=twocolors,\n",
    "        den_cmap = \"YlGnBu_r\",\n",
    "        d_alpha=1.0,\n",
    "        levels=None,#np.linspace(0.02, 1.0, 19),\n",
    "        fname_vs_e = \"./figs/lambda_vs_e_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Discrete levels\n",
    "do_scatter=True\n",
    "contour_label=False\n",
    "surf = False\n",
    "\n",
    "\n",
    "import itertools\n",
    "for levels in [np.linspace(0.01, 1.0, 19), None]:\n",
    "    for combination in itertools.product(*[(True, False)]*3):\n",
    "        do_scatter, contour_label, surf = combination\n",
    "        do_plot(x,y, \n",
    "                do_scatter=do_scatter,\n",
    "                contour_label=contour_label,\n",
    "                surf = surf,\n",
    "                img_scale = 2.0,\n",
    "                twocolors=twocolors,\n",
    "                den_cmap = \"PuBu\",\n",
    "                levels=levels,\n",
    "                fname_vs_e = \"./figs/lambda_vs_e_z0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
