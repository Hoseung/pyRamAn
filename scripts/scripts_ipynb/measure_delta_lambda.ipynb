{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_pickle(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "## time\n",
    "def aexp2zred(aexp):\n",
    "    return [1.0/a - 1.0 for a in aexp]\n",
    "\n",
    "def zred2aexp(zred):\n",
    "    return [1.0/(1.0 + z) for z in zred]\n",
    "\n",
    "def lbt2aexp(lts):\n",
    "    import astropy.units as u\n",
    "    from astropy.cosmology import WMAP7, z_at_value\n",
    "    zreds = [z_at_value(WMAP7.lookback_time, ll * u.Gyr) for ll in lts]\n",
    "    return [1.0/(1+z) for z in zreds]\n",
    "\n",
    "def density_map(x, y, sort=True):\n",
    "    from scipy.stats import gaussian_kde\n",
    "    xy = np.vstack([x,y])\n",
    "    z = gaussian_kde(xy)(xy) \n",
    "    z /= max(z)\n",
    "\n",
    "    idx = z.argsort()    \n",
    "    xx, yy = x[idx], y[idx]\n",
    "    z = z[idx]\n",
    "    \n",
    "    return xx, yy, z\n",
    "\n",
    "\n",
    "def sigma_clip_ind(c, high, low):\n",
    "    \"\"\"\n",
    "        returns indices of sigma-clipping-safe elements.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    ind = (np.mean(c) - np.std(c)*low < c) * (c < np.mean(c) + np.std(c)*high)\n",
    "    return ind\n",
    "\n",
    "\n",
    "def mask_outlier(y, low=1.5, high=1.5):\n",
    "    \"\"\"\n",
    "        maks outlier assuming monotonic trend.\n",
    "    \"\"\"\n",
    "    x = np.arange(len(y))\n",
    "\n",
    "    # linear fitting .. more desirably, a very strong smoothing scheme that can reconstrcut mild curve.\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x,y)\n",
    "\n",
    "    # extract linear fit\n",
    "    yy = y - (slope * x + intercept)\n",
    "\n",
    "    # sigma clipped value = mean of the rest \n",
    "    i_good = sigma_clip_ind(yy, low, high)\n",
    "    yy[~i_good] = np.mean(yy[i_good])\n",
    "\n",
    "    # add linear fit again\n",
    "    return yy + (slope * x + intercept)\n",
    "\n",
    "\n",
    "def smooth(x, beta=5, window_len=20, monotonic=False, clip_tail_zeros=True):\n",
    "    \"\"\" \n",
    "    kaiser window smoothing.\n",
    "    \n",
    "    If len(x) < window_len, window_len is overwritten to be len(x).\n",
    "    This ensures to return valid length fo array, but with modified window size.\n",
    "       \n",
    "    \n",
    "    beta = 5 : Similar to a Hamming\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    if clip_tail_zeros:\n",
    "        x = x[:max(np.where(x > 0)[0])+1]\n",
    "    \n",
    "    if monotonic:\n",
    "        \"\"\"\n",
    "        if there is an overall slope, smoothing may result in offset.\n",
    "        compensate for that. \n",
    "        \"\"\"\n",
    "        slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y=np.arange(len(x)))\n",
    "        xx = np.arange(len(x)) * slope + intercept\n",
    "        x = x - xx\n",
    "    \n",
    "    # extending the data at beginning and at the end\n",
    "    # to apply the window at the borders\n",
    "    window_len = min([window_len, len(x)])\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]] # concatenate along 0-th axis.\n",
    "    # periodic boundary.\n",
    "    w = np.kaiser(window_len,beta)\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    if monotonic: \n",
    "         return y[int(window_len)/2:len(y)-int(window_len/2) + 1] + xx\n",
    "    else:\n",
    "        return y[int(window_len)/2:len(y)-int(window_len/2) + 1]\n",
    "        #return y[5:len(y)-5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MainPrg():\n",
    "    import tree.ctutils as ctu\n",
    "    import numpy as np\n",
    "    \n",
    "    def __init__(self, treedata, final_gal, nout_ini=None, nout_fi=None):\n",
    "\n",
    "        temp_tree = ctu.extract_main_tree(treedata, final_gal)\n",
    "        if nout_ini == None:\n",
    "            nout_ini = min(temp_tree['nout'])\n",
    "        if nout_fi == None:\n",
    "            nout_fi = max(temp_tree['nout'])            \n",
    "            \n",
    "        self.nouts = np.arange(nout_fi, nout_ini -1, -1)\n",
    "        self.idxs = temp_tree['id'] # nout_ini, nout_fi consideration needed.\n",
    "        self.ids = temp_tree['Orig_halo_id']\n",
    "        self.data = None\n",
    "    \n",
    "    def set_data(self, cat, nout):\n",
    "        \"\"\"\n",
    "        compile data from catalogs.\n",
    "        \"\"\"\n",
    "        if nout in self.nouts:\n",
    "            # Declare self.data first if there isn't.\n",
    "            if self.data == None:\n",
    "                self.data = np.zeros(len(self.nouts), dtype=cat.dtype)\n",
    "            inow = self.nouts == nout\n",
    "            a = np.where(cat['idx'] == self.idxs[inow])[0]\n",
    "            if len(a) > 0:\n",
    "                self.data[inow] = cat[a]        \n",
    "            else:\n",
    "                pass\n",
    "                #print(self.ids[inow],cat['id'])\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"No {} in the catalog\".format(nout))\n",
    "            \n",
    "    def clip_non_detection(self):\n",
    "        # end of galaxy tree = last non-zero position.\n",
    "        # Note that 'id' can be 0 if phantom. But phantom is a valid datapoint\n",
    "        i_first_nout = max(np.where(self.data['idx'] > 0)[0])\n",
    "        #print('i_first', i_first_nout)\n",
    "        # then, only [0-i_first_nout] are valid.\n",
    "        # earlier then 187 - 91-th are zero. so get rid of them.\n",
    "        self.data = self.data[:i_first_nout].copy()\n",
    "        self.nouts = self.nouts[:i_first_nout].copy()\n",
    "        self.ids = self.ids[:i_first_nout].copy()\n",
    "        self.idxs = self.idxs[:i_first_nout].copy()\n",
    "        \n",
    "    def fill_missing_data(self):\n",
    "        assert (self.ids[-1] != 0)\n",
    "        # position angles cannot be linearly interpolated.\n",
    "        # skip.\n",
    "        #\n",
    "        # position and velocity are also not that linear..\n",
    "        # but let me just interpolate them. \n",
    "        #\n",
    "        # \n",
    "        # excluded=[\"lambda_arr2\"]\n",
    "        filled_fields = [\"eps\", \"epsh\", \"epsq\", \"lambda_12kpc\",\n",
    "                         \"lambda_arr\", \"lambda_arrh\",\n",
    "                         \"lambda_r\",\"lambda_r12kpc\",\n",
    "                         \"lambda_r2\",\"lambda_rh\",\"mgas\",\"mrj\",\"mstar\",\n",
    "                         \"reff\",\"reff2\",\"rgal\",\"rgal2\",\"rhalo\",\"rscale_lambda\",\n",
    "                         \"sfr\",\"sma\",\"smah\",\"smaq\",\"smi\",\"smih\",\"smiq\",\"ssfr\",\n",
    "                         \"vxc\", \"vyc\", \"vzc\", \"xc\", \"yc\", \"zc\"]\n",
    "\n",
    "        i_good_max = max(np.where(gal.data[\"reff\"] > 0)[0])\n",
    "        i_bad = np.where(gal.data['idx'] == 0)[0]\n",
    "        i_bad = i_bad[i_bad < i_good_max]\n",
    "        if len(i_bad) > 0:\n",
    "            for field in filled_fields:\n",
    "                # do not modify index and id fields.\n",
    "                arr = gal.data[field] # it's a view.\n",
    "\n",
    "                for i_b in i_bad:\n",
    "                    # neighbouring array might also be empty. Search for closest valid element.\n",
    "                    # left point\n",
    "                    i_l = i_b - 1\n",
    "                    while(i_l in i_bad):\n",
    "                        i_l = i_l - 1\n",
    "                    # right point\n",
    "                    i_r = i_b + 1\n",
    "                    while(i_r in i_bad):\n",
    "                        i_r = i_r + 1\n",
    "                    arr[i_b] = (arr[i_b -1] + arr[i_b +1])/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fixed_ind_Lr(gal):\n",
    "    nnouts = len(gal.nouts)\n",
    "    ind_reff_fix = np.zeros(nnouts, dtype='i4')\n",
    "\n",
    "    #print(gal.data['rgal'])\n",
    "    smooth_r = smooth(mask_outlier(gal.data['rgal'], 1.5, 1.5), 50, monotonic=False)\n",
    "\n",
    "    # fixed Reff array\n",
    "    for i in range(nnouts):\n",
    "        # 1Reff = 5 points\n",
    "        reff_real = smooth_r[i]\n",
    "        reff = gal.data['rgal'][i]\n",
    "        try:\n",
    "            ind_reff_fix[i] = np.round(reff_real/reff * 5) -1\n",
    "        except:\n",
    "            pass\n",
    "    return ind_reff_fix\n",
    "\n",
    "\n",
    "def smoothed_reff(cat, nout_merger):\n",
    "    \"\"\"\n",
    "    returns \"representative\" lambda at each nout by assuming monotonic change in Reff. \n",
    "    During merger, Reff can fluctuate, and if has no physical meaning to infer Labda at Reff during merger stage. \n",
    "    So Reff' is derived by linear interpolating Reffs before and after the merger. \n",
    "    \n",
    "    cat is one galaxy catalog over time.\n",
    "    \"\"\"\n",
    "    import utils.match as mtc\n",
    "    i_merger = np.where(cat['nout'] == nout_merger)[0]\n",
    "    ind_lower = 20\n",
    "    ind_upper = 20\n",
    "    \n",
    "    reffs = cat['rgal']\n",
    "    # left and right values chosen by sigma-clipping\n",
    "    r_lefts, b, c = scipy.stats.sigmaclip(reffs[max([0,i_merger-ind_lower]):i_merger], sig_lower, sig_upper)\n",
    "    #print(r_lefts)\n",
    "    r_left = r_lefts[-1]\n",
    "    i_left = np.where(reffs == r_left)[0]\n",
    "\n",
    "    r_rights, b,c = scipy.stats.sigmaclip(reffs[i_merger:min([i_merger+ind_upper,len(reffs)])], sig_lower, sig_upper)\n",
    "    r_right = r_rights[0]\n",
    "    i_right = np.where(reffs == r_right)[0]\n",
    "\n",
    "    r_prime = reffs\n",
    "    #print(\"chekc\")\n",
    "    #print(r_prime)\n",
    "    r_prime[i_left : i_right + 1] = np.linspace(r_left, r_right, i_right - i_left + 1)\n",
    "    return r_prime    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l_at_smoothed_r(gal, npix_per_reff=5):\n",
    "    npix_per_reff = 5\n",
    "    n_valid_points = sum(gal.data[\"reff\"] > 0)\n",
    "    new_l_arr = np.zeros(n_valid_points)\n",
    "    new_reff=smooth(gal.data[\"reff\"][gal.data[\"reff\"] > 0])\n",
    "    for i in range(n_valid_points):\n",
    "        try:\n",
    "            lambdar = gal.data[\"lambda_arr\"][i]\n",
    "            ind_org = npix_per_reff - 1\n",
    "            i_new = new_reff[i]/gal.data[\"reff\"][i] * ind_org # 0-indexed.\n",
    "            il = np.fix(i_new).astype(int)\n",
    "            ir = il + 1\n",
    "            if ir >= len(lambdar):\n",
    "                new_l_arr[i] = lambdar[-1]\n",
    "            else:\n",
    "                new_l_arr[i] = lambdar[il]*(ir-ind_org) + lambdar[ir]*(ind_org-il)\n",
    "        except:\n",
    "            new_l_arr[i] = gal.data[\"lambda_arr\"][i] # = 0 with bad measurements.\n",
    "    return new_l_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mr_dl(cluster, mr, dl):\n",
    "    wdir = base + cluster + '/'\n",
    "\n",
    "    # Serialize catalogs. -> Only main galaxies\n",
    "    # main galaxy list\n",
    "    alltrees = ctu.load_tree(wdir, is_gal=True)\n",
    "    ad = alltrees.data\n",
    "    tn = ad[ad['nout'] == nout_fi]\n",
    "\n",
    "    cat = load_cat(wdir + cdir + 'catalog' + str(nout_fi) + '.pickle')\n",
    "    #idx_all = [tn['id'][tn['Orig_halo_id'] == id_final][0] for id_final in cat['id']]\n",
    "    idx_all = cat['idx'][cat[\"idx\"] > 0].astype(int) # why idx are float???\n",
    "\n",
    "    mpg_tmp = []\n",
    "    for i, idx in enumerate(idx_all):\n",
    "        #print(i, idx)\n",
    "\n",
    "        mpg_tmp.append(MainPrg(ad, idx))\n",
    "#    mpg_tmp =[MainPrg(ad, idx) for idx in idx_all]\n",
    "    for nout in range(nout_ini, nout_fi + 1):\n",
    "        cat = load_cat(wdir + cdir + 'catalog' + str(nout) + '.pickle')\n",
    "        for gal in mpg_tmp:\n",
    "            gal.set_data(cat, nout)\n",
    "#        print(nout)\n",
    "\n",
    "    while len(mpg_tmp) > 0:\n",
    "        mpgs.append(mpg_tmp.pop())\n",
    "\n",
    "    with open('main_prgs_GM.pickle', 'wb') as f:\n",
    "        pickle.dump(mpgs, f)   \n",
    "\n",
    "    #wdir = './' + cluster + '/' #'05427/'\n",
    "    # Serialize catalogs. -> Only main galaxies\n",
    "    # main galaxy list\n",
    "    #alltrees = ctu.load_tree(wdir, is_gal=True)\n",
    "    #ad = alltrees.data\n",
    "    #tn = ad[ad['nout'] == nout_fi] # tree now\n",
    "\n",
    "    #cat = load_cat(wdir + cdir + 'catalog' + str(nout_fi) + '.pickle')\n",
    "    #last_idx_all = cat['idx'] # idx of galaxies at the last snapshot.\n",
    "\n",
    "    # list of Main progenitor class.\n",
    "    #   feed in alltree.data and idx to initialize MainPrg object.\n",
    "    #mpgs = [MainPrg(ad, idx) for idx in last_idx_all] \n",
    "    \n",
    "    # Compile catalogs ##################\n",
    "    #for nout in range(nout_ini, nout_fi + 1):\n",
    "    #    cat = pickle.load(open(wdir + cdir + 'catalog' + str(nout) + '.pickle', 'rb'))\n",
    "    #    for mpg in mpgs:\n",
    "    #        mpg.set_data(cat, nout) # fill up MainPrg data\n",
    "    #    print(nout, end='\\r')\n",
    "        \n",
    "     \n",
    "    # load merger galaxy list (geneated by scripts/notebooks/halo/Merter_no_cat.ipynb) ##################\n",
    "    with open(wdir + 'merger_list.txt', 'rb') as f:\n",
    "        mgl = np.genfromtxt(f, dtype=[('idx','i8'),('mr','f8'),('nout','i4')])\n",
    "\n",
    "    mids = mgl['idx'] # Merger-galaxy IDs\n",
    "    mrs = mgl['mr'] # Merger ratios\n",
    "    nout_mergers = mgl['nout'] # merger epoch\n",
    "    i_mergers = nout_fi - nout_mergers # merger epoch as nout index.\n",
    "\n",
    "    gal_idxs = [gal.idxs[0] for gal in mpgs] # idx of all galaxies from cluster catalog.\n",
    "\n",
    "    \n",
    "    bad = 0\n",
    "    \n",
    "    #1. merger list에 있는 은하들에 대해서 은하 catalog를 업데이트 한다. \n",
    "    #2. 은하의 catalog가 충분히 긴지 확인\n",
    "    #3. Rvir이 요동치는 경우 Reff를 보정. (요동치기 전, 후의 값을 linear interpolate)\n",
    "    #4. 전-후의 lambda를 측정해서 dl vs mr 측정. \n",
    "    for igal, mid in enumerate(mids):\n",
    "    #gal = mpgs[3]\n",
    "        if mid not in gal_idxs:\n",
    "            print(\"Merger gal {} is not in catalog, skipping\".format(mid))\n",
    "            continue\n",
    "        else:\n",
    "            gal = mpgs[np.where(gal_idxs == mid)[0]]\n",
    "\n",
    "        if len(gal.nouts) < 20:\n",
    "            continue\n",
    "        gal.clip_non_detection()\n",
    "        try:\n",
    "            gal.fill_missing_data()\n",
    "        except:\n",
    "            bad = bad + 1\n",
    "            pass\n",
    "\n",
    "\n",
    "        if verbose: print(\"Galaxy ID at the final nout, idx = {}, id = {}\".format(gal.idxs[0], gal.ids[0]))\n",
    "\n",
    "        i_merger = i_mergers[igal]  #i_merger = 187 - mgl[igal][2]\n",
    "        merger_ratio = mrs[igal]\n",
    "\n",
    "        if verbose: print(\"nnouts: {}, i_merger {}\".format(len(gal.nouts), i_merger))\n",
    "\n",
    "        if i_merger > len(gal.nouts):\n",
    "            print(\"Too short evolution history, aborting..\")\n",
    "            continue\n",
    "\n",
    "        # fixed Lambda array based on Reff_fix.\n",
    "        if fix_ind:\n",
    "            ind_reff_fix = fixed_ind_Lr(gal)\n",
    "            lam = np.zeros(len(ind_reff_fix))\n",
    "\n",
    "            ind_max = len(gal.data['lambda_arr'][0]) - 1\n",
    "\n",
    "            for inout, ind in enumerate(ind_reff_fix):#[ind_reff_fix > 0]):\n",
    "                if ind == 0 : print(ind)\n",
    "                lam[inout] = gal.data['lambda_arr'][inout][min([ind_max,ind])] # fixed value\n",
    "        else:\n",
    "            lam = gal.data['lambda_r']\n",
    "\n",
    "        x_al = range(max([0,i_merger-ind_lower]), i_merger) # nout before merger\n",
    "        x_ar = range(i_merger,min([i_merger+ind_upper, len(lam)])) # nout after merger\n",
    "\n",
    "        # representative value of lambda BEFORE the merger\n",
    "        al, b1, c1 = scipy.stats.sigmaclip(lam[x_al], sig_lower, sig_upper) \n",
    "\n",
    "        # representative value of lambda AFTER the merger\n",
    "        ar, b2, c2 = scipy.stats.sigmaclip(lam[x_ar], sig_lower, sig_upper)\n",
    "\n",
    "        if (len(al) > 1) & (len(ar) > 0):\n",
    "            dl.append(np.median(ar) - np.median(al))\n",
    "            mr.append(merger_ratio)\n",
    "        else:\n",
    "            print(\"error in measuring lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05427\n",
      "Loaded an extended tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoseung/mypy/lib/python3.5/site-packages/ipykernel/__main__.py:24: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05420\n",
      "Loaded an extended tree\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-53b1b886eecf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;31m#    mpg_tmp =[MainPrg(ad, idx) for idx in idx_all]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnout\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnout_ini\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnout_fi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_cat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwdir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcdir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'catalog'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgal\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmpg_tmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mgal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoseung/Work/pyclusterevol/analysis/misc.py\u001b[0m in \u001b[0;36mload_cat\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoseung/mypy/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_named_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m                     \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoseung/mypy/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m   5353\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5354\u001b[0m         return _list_of_dict_to_arrays(data, columns,\n\u001b[1;32m-> 5355\u001b[1;33m                                        coerce_float=coerce_float, dtype=dtype)\n\u001b[0m\u001b[0;32m   5356\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5357\u001b[0m         return _list_of_series_to_arrays(data, columns,\n",
      "\u001b[1;32m/home/hoseung/mypy/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_list_of_dict_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m   5473\u001b[0m     \u001b[1;31m# assure that they are of the base dict class and not of derived\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5474\u001b[0m     \u001b[1;31m# classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5475\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0md\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5477\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdicts_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hoseung/mypy/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   5473\u001b[0m     \u001b[1;31m# assure that they are of the base dict class and not of derived\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5474\u001b[0m     \u001b[1;31m# classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5475\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0md\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5477\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdicts_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from analysis.misc import load_cat\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import tree.ctutils as ctu\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read a single galaxy evolution catalog.\n",
    "import pickle\n",
    "\n",
    "verbose=True\n",
    "# In[4]:\n",
    "base = './'\n",
    "cdir = ['catalog/', 'easy/', 'catalog_GM/', \"easy_final/\"][3]\n",
    "\n",
    "clusters = ['05427', '05420', '29172', \\\n",
    "            '29176', '10002',  '36415',\n",
    "            '06098', '39990', '36413', '04466','17891', '07206', '01605', '35663'][:-5]\n",
    "# parameters used for lambda_arr clipping.\n",
    "ind_upper = 20\n",
    "ind_lower = 20\n",
    "sig_upper = 2.0\n",
    "sig_lower = 2.0\n",
    "\n",
    "nout_ini = 70\n",
    "nout_fi = 187\n",
    "\n",
    "bad = 0\n",
    "#ngals_tot = 0\n",
    "\n",
    "#for cluster in clusters:\n",
    "#    wdir = base + cluster + '/'\n",
    "#    # main galaxy list\n",
    "#    cat = load_cat(wdir + cdir + 'catalog' + str(nout_fi) + '.pickle')\n",
    "#    ngals_tot = ngals_tot + len(cat['idx'] > 0) \n",
    "#    # some galaxies without complete tree are in the catalog.\n",
    "#    # exclude them for this analysis.\n",
    "    \n",
    "#nnouts = nout_fi - nout_ini + 1\n",
    "\n",
    "mpgs = []\n",
    "\n",
    "for cluster in clusters:\n",
    "    print(cluster)\n",
    "    wdir = base + cluster + '/'\n",
    "\n",
    "    # Serialize catalogs. -> Only main galaxies\n",
    "    # main galaxy list\n",
    "    alltrees = ctu.load_tree(wdir, is_gal=True)\n",
    "    ad = alltrees.data\n",
    "    tn = ad[ad['nout'] == nout_fi]\n",
    "\n",
    "    cat = load_cat(wdir + cdir + 'catalog' + str(nout_fi) + '.pickle')\n",
    "    #idx_all = [tn['id'][tn['Orig_halo_id'] == id_final][0] for id_final in cat['id']]\n",
    "    idx_all = cat['idx'][cat[\"idx\"] > 0].astype(int) # why idx are float???\n",
    "\n",
    "    mpg_tmp = []\n",
    "    for i, idx in enumerate(idx_all):\n",
    "        #print(i, idx)\n",
    "\n",
    "        mpg_tmp.append(MainPrg(ad, idx))\n",
    "#    mpg_tmp =[MainPrg(ad, idx) for idx in idx_all]\n",
    "    for nout in range(nout_ini, nout_fi + 1):\n",
    "        cat = load_cat(wdir + cdir + 'catalog' + str(nout) + '.pickle')\n",
    "        for gal in mpg_tmp:\n",
    "            gal.set_data(cat, nout)\n",
    "            gal.cluster = int(cluster)\n",
    "#        print(nout)\n",
    "\n",
    "    while len(mpg_tmp) > 0:\n",
    "        mpgs.append(mpg_tmp.pop())\n",
    "\n",
    "    \n",
    "with open('main_prgs_GM.pickle', 'wb') as f:\n",
    "    pickle.dump(mpgs, f)\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "with PdfPages(\"lambda_plots.pdf\") as pdf:\n",
    "    for gal in mpgs:\n",
    "        #print(sum(gal.data[\"reff\"] > 0),len(smooth(gal.data[\"reff\"])))\n",
    "        #smoothed_reff = smooth(gal.data[\"reff\"])\n",
    "        gal.fill_missing_data()\n",
    "        fig, ax = plt.subplots(2, sharex=True)\n",
    "        plt.title(str(gal.cluster) + \"  \" + str(gal.idxs[0]) +\"  \" + str(gal.ids[0]))\n",
    "        ax[0].plot(gal.data[\"reff\"], label=\"org\")\n",
    "        ax[0].plot(smooth(gal.data[\"reff\"], window_len=10), \"r--\", label=\"smoothed\")\n",
    "        ax[1].plot(gal.data[\"lambda_r\"], label=\"org\")\n",
    "        ax[1].plot(smooth(gal.data[\"lambda_r\"], window_len=10), \"r--\", label=\"smoothed\")\n",
    "        new_ls = l_at_smoothed_r(gal, npix_per_reff=5)\n",
    "        ax[1].plot(smooth(new_ls, window_len=10), \"g--\", label=\"smoothed_new\")        \n",
    "        #ax[2].plot(new_ls, label=\"org\")        \n",
    "        plt.legend()\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "\n",
    "gal.idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gal.cluster = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
